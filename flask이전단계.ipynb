{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNreoUksTVinvpFMAqMzYz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeeSeung-Shin/Section3_pj/blob/master/flask%EC%9D%B4%EC%A0%84%EB%8B%A8%EA%B3%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import time\n",
        "import urllib.request\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import collections\n",
        "from wordcloud import STOPWORDS\n",
        "from scipy.sparse import csr_matrix\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "from wordcloud import WordCloud\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pickler"
      ],
      "metadata": {
        "id": "YF3phO_lzAUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbb_Cwr0KENK"
      },
      "outputs": [],
      "source": [
        "def get_kobis_movie_data(key):\n",
        "    \"\"\"\n",
        "    kobis api key값을 받아서 영화 제목을 받는다.\n",
        "\n",
        "    parameter\n",
        "    \n",
        "    - key: kobis api 키\n",
        "    \"\"\"\n",
        "    num=0\n",
        "    # 추출할 colume을 지정해 데이터 프레임을 생성한다.\n",
        "    kobis_df = pd.DataFrame(columns= ['movieNm', 'genreAlt', 'director','company'])\n",
        "    while True:\n",
        "        num+=1\n",
        "        url = f\"http://www.kobis.or.kr/kobisopenapi/webservice/rest/movie/searchMovieList.json?key={key}&curPage={num}&itemPerPage=100\" \n",
        "        try: \n",
        "            request = requests.get(url)\n",
        "            movie = json.loads(request.content)\n",
        "            start =0+100*(num-1)\n",
        "            end = 100*num\n",
        "            for idx in range(start,end):\n",
        "                kobis_df.loc[idx,'movieNm'] = movie['movieListResult']['movieList'][idx%100]['movieNm']\n",
        "                kobis_df.loc[idx,'prdtYear'] = movie['movieListResult']['movieList'][idx%100]['prdtYear']\n",
        "                kobis_df.loc[idx,'genreAlt'] = movie['movieListResult']['movieList'][idx%100]['genreAlt']\n",
        "                kobis_df.loc[idx,'nationAlt'] = movie['movieListResult']['movieList'][idx%100]['nationAlt']\n",
        "                kobis_df.loc[idx,'director'] = [director['peopleNm'] for director in movie['movieListResult']['movieList'][idx%100]['directors']]\n",
        "                kobis_df.loc[idx,'company'] = [company['companyNm'] for company in movie['movieListResult']['movieList'][idx%100]['companys']]\n",
        "        except:\n",
        "            break\n",
        "        \n",
        "        # 에로영화 제외\n",
        "        kobis_df = kobis_df[-kobis_df['genreAlt'].str.contains('에로')] \n",
        "\n",
        "        # 값이 따로 없는 데이터제거\n",
        "        for col in kobis_df.columns:\n",
        "            kobis_df = kobis_df[kobis_df[col].astype('bool') == True]\n",
        "        kobis_df.reset_index(drop=True, inplace=True)\n",
        "    return kobis_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def put_df_to_database(database, table, df):\n",
        "    \"\"\"\n",
        "    특정 database와 table을 입력하면 해당 database의 table에 df를 넣어준다.\n",
        "    만약 table이 존재하지 않는다면 새로 만들어준후 df를 그대로 넣어준다.\n",
        "    paramter\n",
        "    - database\n",
        "      1. movie:\n",
        "        table \n",
        "        - kobis_movie: kobis_api를 통해 얻은 영화 정보 데이터를 넣어준다.\n",
        "        - naver_movie: naver_api를 통해 얻은 영화 정보 데이터를 넣어준다.\n",
        "        - naver_review: naver_api에 있는 영화의 리뷰데이터를 넣어준다.\n",
        "        - modeling_review: 모델링에 사용할 리뷰데이터를 넣어준다.\n",
        "      2. web_service\n",
        "        table\n",
        "        - service_movie: 서비스에 제공할 영화목록과 키워드가 담긴 데이터를 넣어준다.\n",
        "        - genre_keywords :genre별 keyword가 들어간다.\n",
        "    df: 테이터베이스에 넣어줄 데이터프레임\n",
        "    \"\"\"\n",
        "    if ((database =='movie.db') & (table in ['kobis_movie', 'naver_movie', 'naver_review', 'modeling_review'])) | ((database =='web_service.db') & (table in ['service_movie', 'genre_keywords'])) :\n",
        "        conn = sqlite3.connect(database)\n",
        "        cur = conn.cursor()\n",
        "        \n",
        "        try:\n",
        "            df.to_sql(table,conn)\n",
        "        except:\n",
        "            index = cur.execute(f'SELECT index FROM {table} ORDER BY index DESC LIMIT 1').fetchall()[0]\n",
        "            col_tup=['index']+list(df.columns)\n",
        "            tup=tuple(['?']*(len(df.columns)+1))\n",
        "            for idx in range(len(df)):\n",
        "                cur.execute(f'INSERT INTO {table}{col_tup} VALUES {tup}', [index+idx+1]+df.iloc[idx])\n",
        "            conn.commit()\n",
        "    else: \n",
        "        print('''\n",
        "        1. movie:\n",
        "        table \n",
        "        - kobis_movie: kobis_api를 통해 얻은 영화 정보 데이터를 넣어준다.\n",
        "        - naver_movie: naver_api를 통해 얻은 영화 정보 데이터를 넣어준다.\n",
        "        - naver_review: naver_api에 있는 영화의 리뷰데이터를 넣어준다.\n",
        "        - modeling_review: 모델링에 사용할 리뷰데이터를 넣어준다.\n",
        "        2. web_service\n",
        "        table\n",
        "        - service_movie: 서비스에 제공할 영화목록과 키워드가 담긴 데이터를 넣어준다.\n",
        "        - genre_keywords :genre별 keyword가 들어간다.\n",
        "        ''')\n"
      ],
      "metadata": {
        "id": "9cfptdS1W1vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리위한 함수\n",
        "def change_form(l):\n",
        "    for idx,s in enumerate(l):\n",
        "        l[idx]=s.strip().lstrip('[').lstrip(\"'\").rstrip(']').rstrip(\"'\")\n",
        "    return l"
      ],
      "metadata": {
        "id": "Hg8vloAykg58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Naver_movie_info(client_id,client_secret_id,kobis_df):\n",
        "    '''\n",
        "    영화 제목으로 client_id와 client_secret_id, 영화 제목이 담긴 리스트를 받아\n",
        "    네이버 영화 api를 이용해 링크,이미지,출시년도,감독,배우,별점에 대한 정보를 받는다.\n",
        "\n",
        "    parameters\n",
        "    client_id: 네이버 영화 api의 client_id\n",
        "    client_secret_id: 네이버 영화 api의 client_secret_id\n",
        "    movie_title_list: 네이버 영화 api의 영화제목 리스트\n",
        "    '''\n",
        "    \n",
        "    movie_title_list= list(kobis_df['movieNm'])\n",
        "\n",
        "    \n",
        "    js_list=[]\n",
        "\n",
        "    for movie_name in movie_title_list:\n",
        "        num +=1\n",
        "        if num%10==0:\n",
        "            time.sleep(2)\n",
        "            print(num)\n",
        "        client_id = client_id\n",
        "        client_secret = client_secret_id\n",
        "        encText = urllib.parse.quote(movie_name)\n",
        "        url = \"https://openapi.naver.com/v1/search/movie.json?query=\" + encText\n",
        "        request = urllib.request.Request(url)\n",
        "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
        "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
        "        response = urllib.request.urlopen(request)\n",
        "        response_body = response.read()\n",
        "        js_list+=[json.loads(response_body.decode('utf-8'))]\n",
        "    movie_info=pd.DataFrame(columns=js_list[0]['items'][0].keys())\n",
        "    movie_info.drop('subtitle',axis=1, inplace=True)\n",
        "    movie_info['title']= new_service['movieNm']\n",
        "    idx=-1\n",
        "    for movie_json in js_list:\n",
        "        idx+=1\n",
        "        if movie_json['display']==1:\n",
        "            movie_info.loc[idx,'link'] = movie_json['items'][0]['link']\n",
        "            movie_info.loc[idx,'image'] = movie_json['items'][0]['image']\n",
        "            movie_info.loc[idx,'pubDate'] = movie_json['items'][0]['pubDate']\n",
        "            movie_info.loc[idx,'director'] = movie_json['items'][0]['director']\n",
        "            movie_info.loc[idx,'actor'] = movie_json['items'][0]['actor']\n",
        "            movie_info.loc[idx,'userRating'] = movie_json['items'][0]['userRating']\n",
        "        else:\n",
        "            for multi_json in movie_json['items']:\n",
        "                if (multi_json['title'].lstrip('<b>').rstrip('<\\b>')== movie_info.loc[idx,['title']]) & (new_service.loc[idx,['prdtYear']] == float(multi_json['pubDate'])):\n",
        "                    movie_info.loc[idx,'link'] = movie_json['items'][0]['link']\n",
        "                    movie_info.loc[idx,'image'] = movie_json['items'][0]['image']\n",
        "                    movie_info.loc[idx,'pubDate'] = movie_json['items'][0]['pubDate']\n",
        "                    movie_info.loc[idx,'director'] = movie_json['items'][0]['director']\n",
        "                    movie_info.loc[idx,'actor'] = movie_json['items'][0]['actor']\n",
        "                    movie_info.loc[idx,'userRating'] = movie_json['items'][0]['userRating']\n",
        "                    break\n",
        "    # NaN값제거\n",
        "    for col in movie_info.columns:\n",
        "        movie_info = movie_info[movie_info[col].astype('bool') == True]\n",
        "\n",
        "    \n",
        "    movie_info['genre']=kobis_df'genreAlt']\n",
        "    movie_info['nation']=kobis_df['nationAlt']\n",
        "    movie_info['genre']=movie_info['genre'].apply(lambda x: x.split(',')).apply(lambda x:change_form(x)).apply(lambda x:\",\".join(x))\n",
        "    movie_info['nation']=movie_info['nation'].apply(lambda x: x.split(',')).apply(lambda x:change_form(x)).apply(lambda x:\",\".join(x))\n",
        "    \n",
        "    movie_info.reset_index(drop=True, inplace=True)        \n",
        "    return movie_info      "
      ],
      "metadata": {
        "id": "Wer9JKagXVuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_service_review(naver_movie_title,naver_movie_link):\n",
        "    '''\n",
        "    네이버 영화제목과 url을 받아서 review와 별점이 담긴 데이터프레임을 반환한다.\n",
        "    \n",
        "    parameters \n",
        "        naver_movie_title: 네이버 영화제목이 담긴 리스트\n",
        "        naver_movie_link : 네이버 영화 url이 담긴 리스트\n",
        "\n",
        "    '''\n",
        "    \n",
        "    movie_df = pd.DataFrame(columns=['review_text', 'review_star', 'title'])\n",
        "    idx=0\n",
        "    for movie_title, movie_link in list(zip(naver_movie_title,naver_movie_link)):\n",
        "        movie_code= movie_link.split('code=')[1]\n",
        "        page_url = f\"https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword={movie_code}&target=after&page={page_num}\"\n",
        "        page = requests.get(page_url)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        if len(soup.find('div',class_='paging').find('div').find_all('a')) >10:\n",
        "            text_list = []\n",
        "            star_list = []\n",
        "            for page in range(1,10):\n",
        "                page_url = f\"https://movie.naver.com/movie/point/af/list.naver?st=mcode&sword={movie_code}&target=after&page={page_num}\"\n",
        "                page = requests.get(page_url)\n",
        "                soup = BeautifulSoup(page.content, 'html.parser')\n",
        "                score_list=soup.find('table',class_='list_netizen').find('tbody').find_all('tr')\n",
        "                for reple in score_list:\n",
        "                    idx+=1\n",
        "                    movie_df.loc[idx,'review_text'] = reple.find_all('td')[1].select_one('br').next_sibling.strip()\n",
        "                    movie_df.loc[idx,'review_star'] = int(reple.find('div',class_='list_netizen_score').find('em').text)\n",
        "                    movie_df.loc[idx,'movie_title']=movie_title\n",
        "\n",
        "    return movie_df"
      ],
      "metadata": {
        "id": "Y106oN5rzrvR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_modeling_review():\n",
        "    '''\n",
        "    감성분석 모델학습을 위한 최근 리뷰 10000개를 추출해준다.\n",
        "    '''\n",
        "\n",
        "    url = \"https://movie.naver.com/movie/point/af/list.nhn?&page=\"\n",
        "    headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36'}\n",
        "    \n",
        "    score_list = []\n",
        "    comment_list = []\n",
        "    \n",
        "    for page in range(1,1000):\n",
        "        web = requests.get(url+str(page), headers = headers).content\n",
        "        soup = BeautifulSoup( web, 'html.parser')\n",
        "        \n",
        "        star_score_lst = soup.find_all('div',{'class':\"list_netizen_score\"})\n",
        "        for star_score in star_score_lst:\n",
        "            score_list.append(star_score.find('em').text)\n",
        "            \n",
        "        comment_lst = soup.find_all('td', {'class':\"title\"})\n",
        "        for comment in comment_lst:\n",
        "\n",
        "            # br class 다음 문자열을 불러옴. next_sibling\n",
        "            comment_list.append(comment.select_one('br').next_sibling.strip())\n",
        "        \n",
        "        interval = round(random.uniform(0.2, 1.2),2)\n",
        "        time.sleep(interval)\n",
        "        \n",
        "    modeling_review = pd.DataFrame({'review_text': comment_list, 'review_score': score_list})\n",
        "    return modeling_review"
      ],
      "metadata": {
        "id": "ueqOHHnZ4ywM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 위한 함수\n",
        "def apply_regular_expression(text):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]') \n",
        "    result = hangul.sub('', text)  \n",
        "    return result\n",
        "def text_cleaning(text):\n",
        "    hangul = re.compile('[^ a-z A-Z ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
        "    result = hangul.sub('', text)\n",
        "    okt = Okt()  # 형태소 추출\n",
        "    words = okt.morphs(result, norm=True)\n",
        "    words = [x for x in words if len(x) > 1]  # 한글자 키워드 제거\n",
        "    words = [x for x in words if x not in stop_words_lst]  # 불용어 제거\n",
        "    return words\n",
        "def neg_pos(x):\n",
        "    if x >=7:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "lPViwR19URRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modeling(modeling_review_df):\n",
        "    '''\n",
        "    모델링을 위해 수집한 df를 입력받아서 모델링을 하고 현 디렉토리에 pickle파일로 저장\n",
        "    추가로 예측을위한 데이터프레임을 저장해준다.\n",
        "    parameter\n",
        "    modeling_review_df: 모델링을 위해 수집한 리뷰데이터\n",
        "    '''\n",
        "    # 말뭉치 만들기\n",
        "    corpus = \"\".join(modeling_review_df['review_text'].tolist())\n",
        "    # 정규식 적용\n",
        "    corpus_accept = apply_regular_expression(corpus)\n",
        "    raw_pos_tagged = Okt().pos(corpus, norm=True, stem=True)\n",
        "\n",
        "    # set 함수를 사용하여 raw_pos_tagged pos값을 가져오고 중복은 제거한 순수한 pos값을 남긴다.\n",
        "    set_of_tag = set()\n",
        "    for tag in raw_pos_tagged:\n",
        "        set_of_tag.add(tag[1])\n",
        "    \n",
        "    # 평점 4,5,6점 리뷰 제거\n",
        "    modeling_review_df=modeling_review_df[-modeling_review_df['review_star'].isin([4,5,6])]\n",
        "    \n",
        "    #label 생성\n",
        "    modeling_review_df['positiveness'] = modeling_review_df['review_star'].apply(lambda x: neg_pos(x))\n",
        "    #불용어 사전 제작\n",
        "    stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
        "    stop_words = set([x[0] for x in stopwords])\n",
        "    movie_words = {'하다', '보다','있다','없다','너무','이다','영화',\n",
        "            '되다','않다','같다','만들다','그냥','보고','정말',\n",
        "            '가다','들다','진짜','싶다','정도','오다','많다',\n",
        "            '연기','배우','그리고','부분','나다','편이','분들',\n",
        "                '작품','영화','아니다','되는','겁니다','감독','합니다','싶을','같네'}\n",
        "    stop_words = stop_words.union(movie_words)\n",
        "    \n",
        "    # 추후 예측에 사용할 stopword리스트를 넣어준다.\n",
        "    with open('stop_words_lst.pkl','wb') as pickle_file:\n",
        "        pickle.dump(stop_words_lst, pickle_file)    \n",
        "\n",
        "    #단어 전처리\n",
        "    word_cleaned = []\n",
        "\n",
        "    for word in raw_pos_tagged:\n",
        "        if word[1] not in [\"Josa\", \"Eomi\", \"Punctuation\", \"Foreign\", \"Number\", \"Hashtag\", \"URL\",\"PreEomi\"]: \n",
        "            if (len(word[0]) != 1) & (word[0] not in stop_words):\n",
        "                word_cleaned.append(word[0])\n",
        "            else :\n",
        "                stop_words.add(word[0])\n",
        "    \n",
        "    stop_words_lst = list(stop_words)\n",
        "    \n",
        "    vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
        "    bow_vect = vect.fit_transform(modeling_review_df['review_text'].tolist())\n",
        "    word_list = vect.get_feature_names()\n",
        "    count_list = bow_vect.toarray().sum(axis=0)\n",
        "    \n",
        "    x_data = tf_idf_vect\n",
        "    y_data = modeling_review_df['positiveness']\n",
        "    x_train, x_test, y_train, y_test, df_train, df_test = train_test_split(x_data, y_data, modeling_review_df, test_size = 0.3, random_state=0)\n",
        "    x_train.shape, y_train.shape\n",
        "\n",
        "    lr =LogisticRegression(random_state=0)\n",
        "    lr.fit(x_train, y_train)\n",
        "\n",
        "    lr_pred = lr.predict(x_test)\n",
        "    print('val accuracy: %.4f' % accuracy_score(y_test, lr_pred))\n",
        "\n",
        "    lr =LogisticRegression(random_state=0)\n",
        "    lr.fit(x_data, y_data)\n",
        "\n",
        "    #모델 피클링\n",
        "    with open('model.pkl','wb') as pickle_file:\n",
        "        pickle.dump(lr, pickle_file)\n",
        "\n",
        "    coef_pos_index = sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse = True)\n",
        "    coef_neg_index = sorted(((value, index) for index, value in enumerate(lr.coef_[0])), reverse = False)\n",
        "    invert_index_vectorizer = {v: k for k, v in vect.vocabulary_.items()}\n",
        "    df_word_and_numb = pd.DataFrame(list(vect.vocabulary_.items()), columns=['word','word_num'])\n",
        "    df_score_and_numb = pd.DataFrame(coef_pos_index,columns=['score','word_num'])\n",
        "    df_final = pd.merge(df_word_and_numb, df_score_and_numb ,how = 'inner', on = 'word_num')\n",
        "    df_final\n",
        "    # 모델 가중치에 대한 데이터프레임 현 디렉토리에 피클링\n",
        "    with open('df_final.pkl','wb') as pickle_file:\n",
        "        pickle.dump(df_final, pickle_file)"
      ],
      "metadata": {
        "id": "LIDLBB-TUZaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감성분석을 위한 함수\n",
        "def pos_cal(text):\n",
        "    '''\n",
        "    생성한 모델 기반으로 감성분석의 결과를 호출\n",
        "    '''\n",
        "    # input되는 string을 점수로 바꿔주는 작업\n",
        "    test_text = text\n",
        "    okt.pos(test_text, norm=True, stem=True)\n",
        "    clnd_txt = text_cleaning(test_text)\n",
        "\n",
        "    with open('stop_words_lst.pkl','rb') as pickle_file:\n",
        "        stop_words_lst = pickle.load(pickle_file)\n",
        "    with open('df_final.pkl','rb') as pickle_file:\n",
        "        df_final = pickle.load(pickle_file)\n",
        "\n",
        "    # 점수 계산기\n",
        "    sum_of_coef = 0\n",
        "    count_of_sum = 0\n",
        "    for text in clnd_txt:\n",
        "        if text in list(df_final['word']):\n",
        "            sum_of_coef += float(df_final[df_final['word']==text]['score'])\n",
        "            count_of_sum += 1\n",
        "\n",
        "    # 긍정, 부정 판독기\n",
        "    if count_of_sum != 0:\n",
        "        final_score = sum_of_coef/count_of_sum\n",
        "        if final_score >0:\n",
        "            return 1\n",
        "        elif final_score <0 :\n",
        "            return 0\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "6TyNCl3Ceni-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_service_movie_info(movie_info_df, movie_review_df):\n",
        "    '''\n",
        "    서비스 영화리스트 장르별 키워드에 대한 데이터 프레임 반환\n",
        "    paramters\n",
        "    movie_info_df: naver 영화정보 데이터프레임\n",
        "    movie_review_df : naver 리뷰데이터 데이터프레임\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    ## 600개의 영화 추출(모두 사용하면 keyword추출시 매우 많은 시간이 걸린다.)\n",
        "    top300 = movie_info_df.sort_values(by='userRating',ascending=False).iloc[:300]\n",
        "    bottom300 = movie_info_df[movie_info_df['userRating']!=0].sort_values(by='userRating').iloc[:300]\n",
        "    anal_df=pd.concat([top300,bottom300])\n",
        "\n",
        "    if 'index' in anal_df.columns:\n",
        "        anal_df.drop('index',axis=1,inpace=True)\n",
        "    \n",
        "    anal_movie = list(anal_df['title'])\n",
        "    anal_df.reset_index(drop=True, inplace=True)\n",
        "    movie_review_df = movie_review_df[movie_review_df['movie_title'].isin(anal_movie)]\n",
        "    ##movie_review_df에서 아무것도 안써있는 리뷰는 제외해줌\n",
        "    movie_review_df[movie_review_df['review_text']!='']\n",
        "    \n",
        "    corpus = \"\".join(movie_review_df['review_text'].tolist())\n",
        "    corpus_accept = apply_regular_expression(corpus)\n",
        "    okt= Okt()\n",
        "    raw_pos_tagged = okt.pos(corpus, norm=True, stem=True)\n",
        "\n",
        "    positiveness=anal_review['review_text'].apply(lambda x:pos_cal(x))\n",
        "\n",
        "    movie_review_df['positiveness'] = positiveness\n",
        "    \n",
        "    anal_review= movie_review_df.copy()\n",
        "    \n",
        "    if 'index' in anal_review.columns:\n",
        "        anal_review.drop('index',axis=1,inpace=True)\n",
        "\n",
        "    anal_df['pos_per']=None\n",
        "    anal_df['pos_key']=None\n",
        "    anal_df['neg_key']=None\n",
        "\n",
        "    idx = -1\n",
        "    for movieNm in anal_movie:\n",
        "        idx+=1\n",
        "        corpus_pos = \"\".join(anal_review[(anal_review['movie_title']==movieNm)& (anal_review['positiveness']==1)]['review_text'].tolist())\n",
        "        corpus_neg = \"\".join(anal_review[(anal_review['movie_title']==movieNm)& (anal_review['positiveness']==0)]['review_text'].tolist())\n",
        "        corpus_accept_pos = apply_regular_expression(corpus_pos)\n",
        "        corpus_accept_neg = apply_regular_expression(corpus_neg)\n",
        "        okt= Okt()\n",
        "        noun_pos = okt.nouns(corpus_accept_pos)\n",
        "        noun_neg = okt.nouns(corpus_accept_neg)\n",
        "        noun_pos_list = []\n",
        "        for v in noun_pos:\n",
        "            if (len(v)>=2) and (v not in stop_words):\n",
        "                noun_pos_list.append(v)\n",
        "        count_pos=Counter(noun_pos_list)\n",
        "        noun_list_pos = count_pos.most_common(5)\n",
        "\n",
        "        noun_neg_list = []\n",
        "        for v in noun_neg:\n",
        "            if (len(v)>=2) and (v not in stop_words):\n",
        "                noun_neg_list.append(v)\n",
        "        count_neg=Counter(noun_neg_list)\n",
        "        noun_list_neg = count_neg.most_common(5)\n",
        "        anal_df.loc[idx,'pos_per']=anal_review['movie_title']==movieNm]['positiveness'].mean()\n",
        "        anal_df.loc[idx,'pos_key']=\",\".join([i for i,_ in noun_list_pos])\n",
        "        anal_df.loc[idx,'neg_key']=\",\".join([i for i,_ in noun_list_neg])\n",
        "\n",
        "        genre_list = []\n",
        "        for i in anal_df['genre']:\n",
        "            genre_list+=i.split(',') \n",
        "        genre_list = list(set(genre_list))\n",
        "\n",
        "    genre_keywords = pd.DataFrame(columns=['genre','keywords']) \n",
        "    genre_keywords['genre']=genre_list\n",
        "    for idx ,genre in enumerate(genre_list):\n",
        "        key_list=[]\n",
        "        for keywords in anal_df[anal_df['genre'].str.contains(genre)].sort_values('pos_per',ascending=False)['pos_key'].iloc[:20]:\n",
        "            key_list+= keywords.split(',')\n",
        "        genre_keywords.loc[idx, 'keywords'] = \",\".join(list(set(key_list)))\n",
        "        anal_df['actor']=anal_df['actor'].str.replace('<b>','').replace('</b>','')\n",
        "\n",
        "    return anal_df,genre_keywords"
      ],
      "metadata": {
        "id": "dud6V-wsIUcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kobis_df = get_kobis_movie_data(key)\n",
        "movie_info = get_Naver_movie_info(client_id,client_secret_id,kobis_df)\n",
        "naver_movie_title = movie_info['title']\n",
        "naver_movie_link = movie_info['link']\n",
        "\n",
        "\n",
        "movie_df = get_service_review(naver_movie_title,naver_movie_link)\n",
        "modeling_review_df = get_modeling_review()\n",
        "put_df_to_database('movie.db', 'kobis_movie', kobis_df)\n",
        "put_df_to_database('movie.db', 'naver_movie', naver_info)\n",
        "put_df_to_database('movie.db', 'naver_review', movie_df)\n",
        "put_df_to_database('movie.db', 'modeling_review', modeling_revie_df)\n",
        "\n",
        "modeling(modeling_review_df)\n",
        "\n",
        "service_movie,genre_keywords = get_service_movie_info(movie_info, movie_df)\n",
        "put_df_to_database('web_service.db', 'service_movie', service_movie)\n",
        "put_df_to_database('web_service.db', 'genre_keywords', genre_keywords)"
      ],
      "metadata": {
        "id": "gWrtc8rCWAj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}